import numpy as np, pandas as pd
import matplotlib.pyplot as plt, seaborn as sns
from sklearn.metrics import precision_recall_curve, average_precision_score
from sklearn.calibration import CalibratedClassifierCV
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay,
                             roc_auc_score, roc_curve, precision_recall_fscore_support,
                             classification_report)
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE
import shap, warnings; warnings.filterwarnings("ignore")
RANDOM_STATE = 42

# === 1. Cargar datos y mostrar primeras filas ===
df = pd.read_csv(r'C:\\Users\\SISTEMAS\\Documents\\PYTHON\\DATA\\data.csv', sep=';')
print(df.head())

# === 2. Resumen de columnas para el informe (tipo, nulos, %) ===
resumen_cols = pd.DataFrame({
    "tipo": df.dtypes.astype(str),
    "nulos": df.isna().sum(),
    "%_nulos": (df.isna().mean()*100).round(2)
})
resumen_cols.head(20)

# === 3. Preparar variable objetivo binaria (Dropout=1, otros=0) ===
df["target_bin"] = (df["Target"] == "Dropout").astype(int)

# === 4. Separar variables predictoras y objetivo, ignorando IDs si existen ===
id_cols = [c for c in df.columns if c.lower() in ["id","student_id"]]
X = df.drop(columns=["Target","target_bin"] + id_cols, errors="ignore")
y = df["target_bin"]

# === 5. Separar datos en entrenamiento y prueba, manteniendo proporción de desertores ===
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE
)
y_train.mean(), y_test.mean()  # proporción de desertores

# === 6. Identificar columnas numéricas y categóricas para preprocesamiento ===
num_cols = X_train.select_dtypes(include=np.number).columns.tolist()
cat_cols = X_train.select_dtypes(exclude=np.number).columns.tolist()

# === 7. Definir transformaciones: escalado para numéricas y OneHot para categóricas ===
preproc = ColumnTransformer([
    ("num", StandardScaler(), num_cols),
    ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols)
])

# === 8. Función para crear pipeline con preprocesamiento, balanceo y modelo ===
def make_pipe(estimator):
    return ImbPipeline([
        ("prep", preproc),
        ("smote", SMOTE(random_state=RANDOM_STATE)),
        ("clf", estimator),
    ])

# === 9. Definir modelos a comparar ===
models = {
    "LogReg": LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),
    "RF": RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=RANDOM_STATE),
    "SVM": SVC(kernel="rbf", probability=True, random_state=RANDOM_STATE),
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)

# === 10. Evaluar modelos con validación cruzada (AUC y F1) ===
rows = []
for name, est in models.items():
    pipe = make_pipe(est)
    auc = cross_val_score(pipe, X_train, y_train, cv=cv, scoring="roc_auc", n_jobs=-1)
    f1  = cross_val_score(pipe, X_train, y_train, cv=cv, scoring="f1", n_jobs=-1)
    rows.append([name, auc.mean(), auc.std(), f1.mean(), f1.std()])

bench = pd.DataFrame(rows, columns=["Modelo","AUC_mean","AUC_std","F1_mean","F1_std"])\
         .sort_values("AUC_mean", ascending=False)
bench

# === 11. Entrenar el mejor modelo en todo el set de entrenamiento ===
best_name = bench.iloc[0,0]
best_est  = models[best_name]
best_pipe = make_pipe(best_est).fit(X_train, y_train)

# === 12. Predicción y métricas con umbral estándar (0.5) ===
proba = best_pipe.predict_proba(X_test)[:,1]
pred  = (proba >= 0.5).astype(int)

auc = roc_auc_score(y_test, proba)
prec, rec, f1, _ = precision_recall_fscore_support(y_test, pred, average="binary")
print(f"{best_name} | AUC={auc:.3f} | Precision={prec:.3f} | Recall={rec:.3f} | F1={f1:.3f}")

print("\nClassification report:")
print(classification_report(y_test, pred, digits=3))

# === 13. Matriz de confusión y curva ROC ===
cm = confusion_matrix(y_test, pred)
ConfusionMatrixDisplay(cm).plot(cmap="Blues"); plt.title("Matriz de confusión"); plt.show()

fpr, tpr, _ = roc_curve(y_test, proba)
plt.plot(fpr, tpr, label=f"{best_name} (AUC={auc:.3f})")
plt.plot([0,1],[0,1],'k--'); plt.xlabel("FPR"); plt.ylabel("TPR")
plt.title("Curva ROC"); plt.legend(); plt.grid(True); plt.show()

# === 14. Importancia de variables (solo si el modelo lo permite) ===
try:
    ohe = best_pipe.named_steps["prep"].named_transformers_["cat"]
    cat_names = ohe.get_feature_names_out(cat_cols) if len(cat_cols)>0 else []
    feat_names = np.r_[num_cols, cat_names]

    if hasattr(best_pipe.named_steps["clf"], "feature_importances_"):
        imp = best_pipe.named_steps["clf"].feature_importances_
        fi = pd.DataFrame({"feature": feat_names, "importance": imp})\
             .sort_values("importance", ascending=False).head(20)
        sns.barplot(x="importance", y="feature", data=fi); plt.title("Top 20 variables"); plt.show()
except Exception as e:
    print("Importancias no disponibles:", e)

# === 15. Explicabilidad con SHAP (opcional, solo si el modelo lo soporta) ===
try:
    explainer = shap.Explainer(best_pipe.predict_proba, X_train)
    shap_values = explainer(X_test.sample(min(200, len(X_test)), random_state=RANDOM_STATE))
    shap.plots.beeswarm(shap_values, max_display=15)
except Exception as e:
    print("SHAP no disponible:", e)

# === 16. Búsqueda de hiperparámetros para RandomForest ===
param_grid = {
    "clf__n_estimators": [300, 500],
    "clf__max_depth": [None, 10, 20],
}
grid = GridSearchCV(make_pipe(RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1)),
                    param_grid, scoring="roc_auc", cv=cv, n_jobs=-1, verbose=1)
grid.fit(X_train, y_train)
grid.best_params_, grid.best_score_
best_pipe = grid.best_estimator_

# === 17. Curva Precision-Recall y selección de umbral óptimo por recall objetivo ===
proba = best_pipe.predict_proba(X_test)[:, 1]  # Probabilidades de deserción
prec, rec, th = precision_recall_curve(y_test, proba)
ap = average_precision_score(y_test, proba)
print(f"Average Precision (área bajo PR): {ap:.3f}")

# Buscar umbral que garantice recall mínimo y maximice F1
RECALL_MIN = 0.85
rec_th = rec[:-1]      # recortamos para que coincida con th
prec_th = prec[:-1]
mask = rec_th >= RECALL_MIN
if mask.sum() > 0:
    f1_mask = 2 * (prec_th[mask] * rec_th[mask]) / (prec_th[mask] + rec_th[mask] + 1e-12)
    i = f1_mask.argmax()
    th_opt = th[mask][i] if i < len(th[mask]) else th[mask][0]
else:
    f1_all = 2 * (prec_th * rec_th) / (prec_th + rec_th + 1e-12)
    i = f1_all.argmax()
    th_opt = th[i] if i < len(th) else th[0]

print(f"Umbral seleccionado por criterio (recall≥{RECALL_MIN} y F1 máx aprox): {th_opt:.3f}")

# Comparar métricas con umbral estándar (0.50) y óptimo
def eval_with_threshold(thr):
    pred_t = (proba >= thr).astype(int)
    P, R, F1, _ = precision_recall_fscore_support(y_test, pred_t, average="binary", zero_division=0)
    cm = confusion_matrix(y_test, pred_t)
    return P, R, F1, cm

P50, R50, F150, cm50 = eval_with_threshold(0.50)
Popt, Ropt, F1opt, cmopt = eval_with_threshold(th_opt)

print(f"[0.50]  precision={P50:.3f}  recall={R50:.3f}  F1={F150:.3f}")
print(f"[{th_opt:.3f}] precision={Popt:.3f}  recall={Ropt:.3f}  F1={F1opt:.3f}")

# Gráfico Precision-Recall mostrando ambos umbrales
plt.figure(figsize=(6,4))
plt.plot(rec, prec, label=f"PR curve (AP={ap:.3f})")
plt.scatter([R50, Ropt], [P50, Popt], c=["tab:gray","tab:orange"], label="Umbrales: 0.50 y óptimo")
plt.xlabel("Recall"); plt.ylabel("Precision"); plt.title("Curva Precision-Recall")
plt.legend(); plt.grid(True); plt.show()

# Matriz de confusión con el umbral óptimo
ConfusionMatrixDisplay(cmopt).plot(cmap="Blues")
plt.title(f"Matriz de confusión (umbral={th_opt:.3f})")
plt.show()

# === 18. Calibración del modelo (Isotonic) y comparación de métricas ===
cal_rf = CalibratedClassifierCV(
    estimator=RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1),
    method="isotonic",
    cv=5
)

cal_pipe = ImbPipeline(steps=[
    ("prep", preproc),        # mismo preprocesamiento que el pipeline original
    ("smote", SMOTE(random_state=RANDOM_STATE)),
    ("clf", cal_rf)
])

cal_pipe.fit(X_train, y_train)

# Probabilidades calibradas y métricas con umbral 0.5
proba_cal = cal_pipe.predict_proba(X_test)[:,1]
auc_cal = roc_auc_score(y_test, proba_cal)
pred_cal = (proba_cal >= 0.5).astype(int)
Pcal, Rcal, F1cal, _ = precision_recall_fscore_support(y_test, pred_cal, average="binary")

print(f"RF BASE  | AUC={auc:.3f} Precision={float(prec):.3f} Recall={float(rec):.3f} F1={float(f1):.3f}")
print(f"RF CALIB | AUC={auc_cal:.3f} Precision={float(Pcal):.3f} Recall={float(Rcal):.3f} F1={float(F1cal):.3f}")

# Tabla resumen de configuraciones y métricas
tabla = pd.DataFrame([
    ["RF base (0.50)", P50, R50, F150, auc],
    [f"RF base ({th_opt:.2f})", Popt, Ropt, F1opt, auc],
    ["RF calibrado (0.50)", Pcal, Rcal, F1cal, auc_cal],
], columns=["Config","Precision","Recall","F1","AUC"])
tabla

# === 19. Guardar artefactos: modelo, benchmark y resumen de variables ===
import joblib, os
os.makedirs("artifacts", exist_ok=True)
joblib.dump(best_pipe, "artifacts/modelo_desercion.pkl")
bench.to_csv("artifacts/benchmark_modelos.csv", index=False)
resumen_cols.to_csv("artifacts/resumen_variables.csv")
